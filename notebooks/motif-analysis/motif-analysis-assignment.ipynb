{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Analysis of DNA Sequence Motifs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.typing as npt\n",
    "from typing import Any, Callable, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise walks you through the application of a convolutional neural network (CNN) to DNA sequence data.\n",
    "\n",
    "We will use PyTorch as a neural network framework.\n",
    "While there is no shortage of content and tutorials on how to get started with PyTorch in general, in most cases images or language samples are taken as input data.\n",
    "For using DNA sequences as an input, there are many great projects out there that have developed PyTorch frameworks to model all sorts of biological phenomena.\n",
    "Examples include [Selene](https://github.com/FunctionLab/selene), [Basset](https://github.com/davek44/Basset), and [DNABERT](https://github.com/jerryji1993/DNABERT) as well as [DNABERT-2](https://github.com/Zhihan1996/DNABERT_2).\n",
    "However, thewe frameworks can be quite sophisticated and difficult to dive into.\n",
    "\n",
    "Hence, here we provide a more appropriate starting point for all future DNA modelers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of synthetic DNA data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually scientists might be interested in predicting something like a binding score, an expression strength, or identifying transcription factor binding events.\n",
    "But here, we are going to keep it simple: our goal is to observe if a deep learning model can learn to detect a very small, simple pattern in a DNA sequence and score it appropriately.\n",
    "\n",
    "So arbitrarily, let's say that given an 8-mer DNA sequence, give it points for each base as follows:\n",
    "\n",
    "- A = +20 points\n",
    "- C = +17 points\n",
    "- G = +14 points\n",
    "- T = +11 points\n",
    "\n",
    "Then, for every 8-mer, sum up its total points and then take the average.\n",
    "For example, `AAAA` would score $\\frac{20+20+20+20}{4}=20$.\n",
    "`ACAA` would score $\\frac{20+17+20+20}{4}=19.25$.\n",
    "\n",
    "> Keep in mind that hese values are arbitrary; we are only building our own simple system for demonstration purposes.\n",
    "\n",
    "Let's add one more thing to our scoring system.\n",
    "To simulate something like [motifs](https://en.wikipedia.org/wiki/Sequence_motif) influencing gene expression, a given sequence gets a +10 bump if `TAT` appears anywhere in the 8-mer, and a -10 bump if it has a `GCG` in it.\n",
    "\n",
    "> A sequence motif is a nucleotide or amino-acid sequence pattern that is widespread and usually assumed to be related to biological function of the macromolecule.\n",
    "> DNA sequence motifs are often represented as a [sequence logo](https://en.wikipedia.org/wiki/Sequence_logo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ **Q1** &mdash; Scoring system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following function to score a DNA sequence according to the scheme defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def motif_score(sequence: str) -> float:\n",
    "    base_scores = {\"A\": 20, \"C\": 17, \"G\": 14, \"T\": 11}\n",
    "\n",
    "    # Single-base scoring\n",
    "    # YOUR CODE\n",
    "\n",
    "    # Motif scoring\n",
    "    # YOUR CODE\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"AAAA\"\n",
    "print(f\"motif_score({sequence}) = {motif_score(sequence=sequence)}\")\n",
    "sequence = \"ACAA\"\n",
    "print(f\"motif_score({sequence}) = {motif_score(sequence=sequence)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ **Q2** &mdash; Data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that _k_-mers are substrings of length $k$ contained within a biological sequence.\n",
    "Also, a sequence of length $L$ has $L - k + 1$ _k_-mers and $n^k$ total possible _k_-mers, where $n$ is the number of possible monomers.\n",
    "\n",
    "The following function generates a list of all possible _k_-mers for a given _k_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "\n",
    "def generate_all_kmers(k: int) -> List[str]:\n",
    "    return [\"\".join(x) for x in product([\"A\", \"C\", \"G\", \"T\"], repeat=k)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate all possible $4^8 = 65,536$ 8-mers (i.e., $n = 4$, $k = 8$).\n",
    "Then, compute the motif score for each 8-mer.\n",
    "Finally, store the results in a [`pandas.DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) with the columns `sequence` and `motif_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# YOUR CODE\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set & validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ **Q3** &mdash; Data split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first: split the data randomly into a training set (80%) and a validation set (20%).\n",
    "\n",
    "> Feel free to use the [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# YOUR CODE\n",
    "\n",
    "print(f\"Training set{os.linesep}------------\")\n",
    "print(f\"Dataframe shape: {train_df.shape}\")\n",
    "print(f\"Dataframe head:{os.linesep}{train_df.head()}\")\n",
    "print(\"\")\n",
    "print(f\"Validation set{os.linesep}--------------\")\n",
    "print(f\"Dataframe shape: {val_df.shape}\")\n",
    "print(f\"Dataframe head:{os.linesep}{val_df.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that for neural networks to make predictions, in general you have to give it your input as a matrix of numbers.\n",
    "While for example images are by definition already matrices of numbers, we have to put in some extra work for our DNA sequences.\n",
    "\n",
    "We will use one-hot encoding for our DNA sequences.\n",
    "The following function performs the one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(sequence: str) -> npt.NDArray[Any]:\n",
    "    code = {\n",
    "        \"A\": [1.0, 0.0, 0.0, 0.0],\n",
    "        \"C\": [0.0, 1.0, 0.0, 0.0],\n",
    "        \"G\": [0.0, 0.0, 1.0, 0.0],\n",
    "        \"T\": [0.0, 0.0, 0.0, 1.0],\n",
    "        \"N\": [0.0, 0.0, 0.0, 0.0],\n",
    "    }\n",
    "    return np.array([code[base] for base in sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"GATTACA\"\n",
    "print(f\"one_hot_encode({sequence}) =\")\n",
    "print(f\"{one_hot_encode(sequence=sequence)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a PyTorch dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, a dataset is represented by a regular Python class that inherits from the [Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class.\n",
    "You can think of it as a kind of a Python _list of tuples_, each tuple corresponding to one data point (features, label).\n",
    "\n",
    "The most fundamental methods it needs to implement are:\n",
    "\n",
    "- `__init__()`: it takes whatever arguments needed to build a list of tuples. This may be a CSV file that will be loaded and processed; it may be two tensors, one for features, another one for labels; or anything else, depending on the task at hand. However, there is no need to load the entire dataset here in the constructor. Indeed, if your dataset is big it is recommended to load data on demand, i.e., whenever `__get_item__()` is called.\n",
    "- `__get_item__()`: it allows the dataset to be indexed, so that it can work like a list (`dataset[i]`). It must return a tuple (features, label) corresponding to the requested data point. We can either return the corresponding slices of our pre-loaded dataset or, as mentioned above, load them on demand.\n",
    "- `__len__()`: it should simply return the size of the entire dataset.\n",
    "\n",
    "Then, with a PyTorch dataset available, we have the opportunity of using PyTorch's [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class for loading our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ **Q4** &mdash; Building a custom PyTorch dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build our own simple custom dataset that takes our dataframe (with the columns `sequence` and `motif_score`) as input. Complete the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "class MotifScoreDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame) -> None:\n",
    "        # Features\n",
    "        sequences = list(df[\"sequence\"].values)\n",
    "        self.ohe_sequences = torch.stack(\n",
    "            tensors=[torch.tensor(data=one_hot_encode(sequence=s)) for s in sequences]\n",
    "        )  # this will be a (n_items x sequence_len x ohe_len) tensor\n",
    "\n",
    "        # Labels\n",
    "        motif_scores = list(df[\"motif_score\"].values)\n",
    "        self.motif_scores = torch.tensor(data=motif_scores).unsqueeze(\n",
    "            dim=1\n",
    "        )  # this will be a (n_items x 1) tensor\n",
    "\n",
    "    def __getitem__(self, index: int) -> (torch.Tensor, torch.Tensor):\n",
    "        # YOUR CODE\n",
    "        return features, label\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.ohe_sequences.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training and validation dataset from our dataframes\n",
    "train_dataset = # YOUR CODE\n",
    "val_dataset = # YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the first item from the training dataframe\n",
    "train_df_head = train_df.head(n=1)\n",
    "train_df_ohe_sequence = one_hot_encode(sequence=train_df_head.iloc[0][\"sequence\"])\n",
    "train_df_motif_score = train_df_head.iloc[0][\"motif_score\"]\n",
    "print(f\"OHE sequence from dataframe:{os.linesep}{train_df_ohe_sequence}{os.linesep}\")\n",
    "print(f\"Motif score from dataframe: {train_df_motif_score}{os.linesep}\")\n",
    "\n",
    "# Inspect the first item from the training dataset\n",
    "datapoint_features, datapoint_label = train_dataset[0]\n",
    "print(f\"OHE sequence from dataset:{os.linesep}{datapoint_features}{os.linesep}\")\n",
    "print(f\"Motif score from dataset: {datapoint_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of a CNN model in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use a CNN that consists of four layers:\n",
    "\n",
    "1. [Conv1d](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html): a 1D convolution over our sequences\n",
    "2. [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html): the rectified linear unit (ReLU) activation function\n",
    "3. [Flatten](https://pytorch.org/docs/stable/generated/torch.flatten.html): to flatten the data into a one-dimensional tensor\n",
    "4. [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html): a final linear transformation to infer an estimate for the motif score\n",
    "\n",
    "We will use PyTorch's [Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) class to build a network in which the layers are automatically connected.\n",
    "\n",
    "Before implementing the CNN, we need to take a more detailed look into the convolutional layer:\n",
    "\n",
    "- We will input our one-hot encoded sequences (8-mers) into the convolutional layer.\n",
    "- Hence, the convolutional layer will have 4 input channels, where each channel corresponds to a specific position of a one-hot encoding. (Recall that our one-hot encodings have the length 4, because we encode the 4 bases A, C, G, and T. E.g., A is encoded as `[1 0 0 0]`.)\n",
    "- We will use 32 filters, i.e., kernels, in our convolutional layer.\n",
    "- Each filter/kernel will have a size $K$ of 3.\n",
    "- Hence, for each 4-channel input sequence of length $L$, the convolutional layer will produce a 32-channel output sequence of length $L - K + 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ **Q5** &mdash; Definition of a CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following code for a CNN model with the parameters as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MotifScoreCNN(nn.Module):\n",
    "    def __init__(self, sequence_len: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        n_input_channels = # YOUR CODE\n",
    "        n_filters = # YOUR CODE\n",
    "        kernel_size = # YOUR CODE\n",
    "        conv_out_len = # YOUR CODE\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=n_input_channels,\n",
    "                out_channels=n_filters,\n",
    "                kernel_size=kernel_size,\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=n_filters * conv_out_len, out_features=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.float:\n",
    "        # Our data loader provides batches with dimensions\n",
    "        # (n_items x sequence_len x ohe_len). We hence have to switch the\n",
    "        # second and third dimension before passing the data to our network.\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ **Q6** &mdash; Training and validation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training process, we expose our CNN model multiple times (`n_epochs` times, to be precise) to the entire dataset.\n",
    "In each epoch, the `train_step()` and `val_step()` functions will be called.\n",
    "In each of these functions we iterate in batches through our training and validation sets.\n",
    "\n",
    "Complete the following code for the training and validation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "\n",
    "\n",
    "def make_train_step(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    loss_fn: torch.nn.modules.loss._Loss,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: str,\n",
    ") -> Callable:\n",
    "    def train_step() -> float:\n",
    "        model.train()\n",
    "        losses = []\n",
    "\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            # Send the batches to the device where the model \"lives\"\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            # Forward pass & loss\n",
    "            y_batch_hat = model(x_batch.float())\n",
    "            loss = loss_fn(y_batch_hat, y_batch.float())\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Gradients & parameter update\n",
    "            # YOUR CODE\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        return np.mean(losses)\n",
    "\n",
    "    return train_step\n",
    "\n",
    "\n",
    "def make_val_step(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    loss_fn: torch.nn.modules.loss._Loss,\n",
    "    device: str,\n",
    ") -> Callable:\n",
    "    def val_step() -> float:\n",
    "        model.eval()\n",
    "        losses = []\n",
    "\n",
    "        # Even though it won't make a difference in our simple model, it is\n",
    "        # good practice to wrap the validation inner loop with this context\n",
    "        # manager to disable any gradient calculation that you may\n",
    "        # inadvertently trigger. Gradients belong in training, not in\n",
    "        # validation.\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in dataloader:\n",
    "                # Send the batches to the device where the model \"lives\"\n",
    "                # YOUR CODE\n",
    "\n",
    "                # Forward pass & loss\n",
    "                # YOUR CODE\n",
    "\n",
    "        return np.mean(losses)\n",
    "\n",
    "    return val_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ **Q7** &mdash; Running the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, everything is set up to run our model.\n",
    "\n",
    "Complete the following code.\n",
    "Pay attention to track the training and validation losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available on your system.\")\n",
    "    device = \"gpu\"\n",
    "else:\n",
    "    print(\"CUDA is not available on your system.\")\n",
    "\n",
    "lr = 0.01\n",
    "sequence_len = len(train_df[\"sequence\"].values[0])\n",
    "model = MotifScoreCNN(sequence_len=sequence_len).to(device=device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "n_epochs = 25\n",
    "\n",
    "train_step = make_train_step(\n",
    "    model=model,\n",
    "    dataloader=train_dataloader,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# YOUR CODE\n",
    "\n",
    "# YOUR CODE\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_step()\n",
    "    # YOUR CODE\n",
    "\n",
    "    val_loss = val_step()\n",
    "    # YOUR CODE\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{(epoch + 1):2}/{n_epochs:2}] | train loss: {train_loss:.4f} | validation loss: {val_loss:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ **Q8** &mdash; Visualization of training and validation losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can inspect the training and validation losses visually.\n",
    "Plot the training and validation losses in a single figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# YOUR CODE\n",
    "plt.xlabel(xlabel=\"Epoch\")\n",
    "plt.ylabel(ylabel=\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amlg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
