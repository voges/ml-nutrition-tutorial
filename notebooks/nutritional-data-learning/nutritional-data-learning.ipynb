{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exploring a Multimodal Nutritional Data Set**\n",
    "\n",
    "This exercise provides a practical example of how a dataset from a real nutritional study could be analyzed and explored using basic statistical methods and machine learning approaches. The primary goal is to identify features that significantly influence an observed nutritional effect within the study. By applying these techniques, we aim to uncover the key factors that contribute to the outcomes, helping to enhance our understanding of nutritional impacts and guide future research. Specifically the small size of the dataset is a challenge here.\n",
    "\n",
    "## The Dataset\n",
    "\n",
    "The dataset we are analyzing is derived from a 2023 study titled \"Distinct Microbial Taxa Are Associated with LDL-Cholesterol Reduction after 12 Weeks of Lactobacillus plantarum Intake in Mild Hypercholesterolemia: Results of a Randomized Controlled Stud\" by Müller et al. This research investigated the effects of probiotic bacteria on individuals with mildly elevated LDL cholesterol levels. Key data collected includes LDL cholesterol levels before and after the 12-week study period, along with various patient features recorded at the study's onset.\n",
    "\n",
    "Our analytical goal is to identify which key features might be linked to a significant reduction in LDL cholesterol. Data from the placebo group or from participants who left the study during execution have already been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "### Basic Exploration\n",
    "\n",
    "In this section, we start by identifying the features of the multimodal nutritional dataset and determining the number of samples. Understanding the structure and volume of our dataset is essential for guiding our preprocessing and analysis efforts.\n",
    "We will analyze how features in our dataset are correlated by generating a heat map. This color-coded representation will help us easily identify strong correlations, revealing potential relationships between different features.\n",
    "Identifying correlated features is crucial because we plan to use a random forest model to infer feature importance. Highly correlated features can lead to inconsistent results in such models due to redundancy. To maintain the robustness of our analysis, we will remove or combine features that are highly correlated.\n",
    "\n",
    "**Task**: Calculate the Pearson [correlation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html) coefficients for each pair of features and plot them as a [heat map](https://seaborn.pydata.org/generated/seaborn.heatmap.html)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def load_dataset(filename):\n",
    "    return pd.read_csv(filename).replace(\"NA\", pd.NA)\n",
    "\n",
    "def plot_correlation(dataset, correlation_threshold):\n",
    "    # Your code: total_correlation_matrix = ??\n",
    "    high_correlation_matrix = total_correlation_matrix[total_correlation_matrix > correlation_threshold][total_correlation_matrix != 1.0]\n",
    "\n",
    "    upper_right_mask = np.triu(np.ones_like(high_correlation_matrix, dtype=bool))\n",
    "\n",
    "    sns.set(font_scale=1.5)\n",
    "    # Your code!\n",
    "    plt.rcParams['figure.figsize'] = [30, 30]\n",
    "    plt.show()\n",
    "\n",
    "dataset = load_dataset(filename='responder.csv')\n",
    "plot_correlation(dataset=dataset, correlation_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Redundant Features\n",
    "\n",
    "As observed from the heat map, our dataset contains several highly correlated features. Some of these correlations are quite straightforward, such as total cholesterol being correlated with LDL and HDL cholesterol levels. Others, while less direct, are still plausible; for example, arterial stiffness shows a correlation with age, and dietary energy intake varies significantly with sex.\n",
    "\n",
    "Given these observations, our next step in preprocessing is to streamline the dataset by removing redundant features. By eliminating features that are already expressed through others, we can simplify the model's complexity without losing crucial information. This reduction not only enhances the performance of our future analyses but also helps in achieving more accurate and interpretable results from the random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_redundant_features(dataset):\n",
    "    labels = dataset.ldl_cholesterin_reduction\n",
    "    redundant_features = \\\n",
    "    ['arterial_stiffness',                              # Already described by age\n",
    "     'waist_hip_ratio', 'diet_energy',                  # Already described by sex\n",
    "     'diet_mediterenian_score',                         # Already described by healthy eating score\n",
    "     'blood_pressure_systolic',                         # Already described by diastolic blood pressure\n",
    "     'diet_carb', 'diet_unsaturated_fat',               # Already described by diet fat\n",
    "     'blood_ldl_cholesterol', 'blood_hdl_cholesterol',  # Already described by total cholesterol\n",
    "     'ldl_cholesterin_reduction'                        # Remove label to prevent data leakage\n",
    "    ]\n",
    "    return pd.DataFrame(labels), dataset.drop(redundant_features, axis=1, errors='ignore')\n",
    "\n",
    "dataset_labels, dataset_redundancy_removed = remove_redundant_features(dataset=dataset)\n",
    "print(\"The columns (n={}) are:\\n{}\\n\".format(len(dataset_redundancy_removed.columns.tolist()), dataset_redundancy_removed.columns.tolist()))\n",
    "#plot_correlation(dataset=dataset_redundancy_removed, correlation_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Imputation\n",
    "Handling missing data is a common challenge in dataset preparation, especially when preparing data for training models. Ideally, we could exclude all samples with missing features to ensure data cleanliness, but this is not feasible due to the already limited size of our dataset. Removing too many samples could significantly impact the validity and generalizability of our model.\n",
    "\n",
    "As an alternative, we will employ data imputation to fill in the missing values using the existing data, ensuring minimal distortion of the dataset. This method allows us to retain as much data as possible while addressing the gaps in information.\n",
    "\n",
    "There are various techniques for data imputation, but for simplicity and effectiveness, we will use the median value of each feature to replace missing values. Using the median is robust against outliers and helps maintain the distributional characteristics of the dataset, making it a suitable choice for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nan(dataset):\n",
    "    # Count the number of NaN values in each column\n",
    "    nan_counts = dataset.isna().sum()\n",
    "\n",
    "    # Filter and print the counts where they are not zero\n",
    "    print(\"Counts of 'NaN' values in each column where they exist:\")\n",
    "    for column, count in nan_counts.items():\n",
    "        if count > 0:\n",
    "            print(f\"{column}: {count}\")\n",
    "\n",
    "def impute_dataset(dataset):\n",
    "    # Compute the mean of each numeric column, excluding NaNs\n",
    "    column_means = dataset.median()\n",
    "\n",
    "    # Fill NaNs with the mean of their respective column\n",
    "    return dataset.fillna(column_means)\n",
    "\n",
    "count_nan(dataset=dataset_redundancy_removed)\n",
    "dataset_imputed = impute_dataset(dataset_redundancy_removed)\n",
    "count_nan(dataset=dataset_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Distribution and Range of Features\n",
    "To better understand the distribution and range of values for each feature in our dataset, we will utilize boxplots. Boxplots are an excellent tool for visually summarizing the central tendency, variability, and presence of outliers within each feature.\n",
    "\n",
    "By analyzing these boxplots, we can quickly gauge the spread of the data, identify any features with extreme values, and determine how these outliers might affect the overall analysis. This visualization step is crucial for ensuring that we fully comprehend the data's characteristics before proceeding with more complex analyses and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_statistics(dataset):\n",
    "    print(\"Distribution statistics for numeric columns:\")\n",
    "    print(dataset.agg(['min', 'max', 'mean', 'median', 'std']))\n",
    "\n",
    "def plot_dataset_boxplots(dataset):\n",
    "    # Plot a combined box plot for all standardized numeric columns\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    dataset.plot(kind='box', vert=False)\n",
    "    plt.title('Standardized Columns')\n",
    "    plt.xlabel('Standardized Value')\n",
    "    plt.show()\n",
    "    \n",
    "plot_dataset_boxplots(dataset_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "The analysis of boxplots has revealed significant variation in the range of values across different features in our dataset. For instance, the feature representing sex ranges from 0 to 1, while age is predominantly above 60. These disparities in scale can pose challenges in modeling, as features with larger ranges might disproportionately influence the results.\n",
    "\n",
    "To address this issue and ensure each feature contributes equally to the analysis, we will standardize the features. Standardization involves rescaling the data so that each feature has a mean of zero and a standard deviation of approximately one. This process transforms each feature into a scale where the values represent \"more of that feature\" or \"less of that feature.\"\n",
    "\n",
    "By standardizing the features, we eliminate the problem of differing scales and make the dataset more suitable for sophisticated statistical techniques and machine learning models, which often assume data is normally distributed around zero. This step is crucial for fair comparison and effective integration of features in our analyses.\n",
    "\n",
    "**Task**: Use the sklearn [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to scale the dataset!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(dataset):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    # Your Code!\n",
    "\n",
    "def preprocess_dataset(filename):\n",
    "    ds = load_dataset(filename)\n",
    "    return remove_redundant_features(ds)\n",
    "\n",
    "def clean_dataset(ds):\n",
    "    ds = impute_dataset(ds)\n",
    "    return scale_dataset(ds)\n",
    "\n",
    "plot_dataset_boxplots(clean_dataset(preprocess_dataset(\"responder.csv\")[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferring Feature Importance by Utilizing a Random Forest\n",
    "### Decision Trees\n",
    "\n",
    "A decision tree is a machine learning model that uses a series of decisions to split data into branches and leaves. At each node of the tree, a decision is made based on a feature, directing the data down the appropriate branch until a prediction is made at a leaf node.\n",
    "\n",
    "This model is straightforward and mimics human decision-making, making it easy to understand and interpret. However, decision trees can overfit if they grow too deep or complex, meaning they may not generalize well to new data. They are versatile for both classification and regression tasks but are often used within ensemble methods like Random Forest to enhance their accuracy and robustness.\n",
    "\n",
    "### Random Forests\n",
    "\n",
    "A Random Forest is an ensemble machine learning method that improves upon the simplicity of decision trees by creating a \"forest\" of them and aggregating their outputs. This model works by constructing multiple decision trees during training, each on a slightly different subset of the data and features. This technique, known as bootstrap aggregating or \"bagging,\" helps in reducing the model's variance.\n",
    "\n",
    "When making predictions, a Random Forest takes the outputs from all the individual trees and combines them to produce a more accurate and stable prediction. For classification tasks, this typically means taking a majority vote among the predictions from all trees. For regression tasks, it usually involves averaging the outputs.\n",
    "\n",
    "The strength of a Random Forest lies in its ability to perform well on complex datasets with minimal tuning while avoiding overfitting, a common problem with individual decision trees. This robustness, combined with its straightforward interpretability (where the importance of features can be easily evaluated), makes Random Forest a popular choice for many predictive modeling tasks. (Image: \"[Random forest explain](https://commons.wikimedia.org/wiki/File:Random_forest_explain.png?uselang=de)\" by \"TseKiChun\" under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/deed.de) license.)\n",
    "\n",
    "![title](Random_forest_explain.png)\n",
    "\n",
    "### Feature Importance\n",
    "\n",
    "Feature importance in a Random Forest provides insights into which features are most influential in making predictions. This method evaluates the impact of each feature on the accuracy of the model by observing how random alterations to feature values affect the model's performance.\n",
    "\n",
    "In a Random Forest, feature importance is typically measured by the average decrease in impurity (such as Gini impurity or entropy) across all trees in the forest when a feature is used to split the data. Alternatively, it can be assessed by examining the increase in prediction error when values of a feature are permuted, which disrupts the relationship between the feature and the target.\n",
    "\n",
    "This approach helps in identifying features that contribute significantly to the decision-making process of the model, allowing for better understanding and optimization of the model. Feature importance is valuable for feature selection, providing a way to remove irrelevant or less important features and potentially improve model performance.\n",
    "\n",
    "### Building a Model to Predict LDL-Cholesterol Reduction\n",
    "\n",
    "To develop a predictive model for LDL cholesterol reduction based on our nutritional study data, we will follow several key steps:\n",
    "\n",
    "#### Splitting the Dataset into Training and Test Sets\n",
    "Before training our model, we must split the dataset into a training set and a test set. The training set is used to train the model, allowing it to learn from the data. The test set, however, serves an unbiased role in evaluating the performance of the model. This separation helps in detecting overfitting and assessing the generalizability of the model to new, unseen data.\n",
    "**It's crucial to apply preprocessing steps to the training and test sets independently**. This approach prevents data leakage, where information from the test set inadvertently influences the model training process. \n",
    "#### Training the Random Forest Classifier\n",
    "\n",
    "With our data divided and preprocessed, we will proceed to train a Random Forest classifier. This model will use the training set to learn the patterns that correlate various dietary and health-related features with reductions in LDL cholesterol.\n",
    "After training, we will assess the performance of the model by calculating the loss on both the training set and the test set. This evaluation helps us understand how well the model has learned from the training data and how it performs on unseen data.\n",
    "\n",
    "#### Extracting Feature Importance\n",
    "Finally, we will extract the feature importances determined by the Random Forest. These importances reveal which features have the most influence on predicting LDL cholesterol reduction. We will visualize these importances using a plot to easily identify the most significant features according to the model. This insight is valuable for understanding the driving factors behind LDL cholesterol reduction and could inform further research and decision-making.\n",
    "\n",
    "**Task**: There are two lines missing from the code below. Complete the code to train the [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) and to compute the [validation loss](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_dataset(dataset,labels,test_size):\n",
    "    train, test, train_labels, test_labels = train_test_split(dataset,labels,test_size=test_size)\n",
    "    test = clean_dataset(pd.DataFrame(test))\n",
    "    test_labels = clean_dataset(test_labels)\n",
    "    return train, test, train_labels, test_labels\n",
    "\n",
    "\n",
    "labels, dataset = preprocess_dataset(\"responder.csv\")\n",
    "\n",
    "train_set, test_set, train_labels, test_labels = split_dataset(dataset,labels,test_size=0.001)\n",
    "train_set, validation_set, train_labels, validation_labels = split_dataset(train_set,train_labels,test_size=0.25)\n",
    "train_set = clean_dataset(train_set)\n",
    "train_labels = clean_dataset(train_labels)\n",
    "\n",
    "def predict_random_forest(train_set, train_labels, validation_set, validation_labels, n_estimators, max_depth):\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    random_forest = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth)\n",
    "    # Your code: one line to train the random forest\n",
    "    training_loss = random_forest.score(train_set, train_labels)\n",
    "    # Your code: one line to calculate the validation loss of the random forest as mean squared error.\n",
    "    return training_loss, validation_loss, random_forest.feature_importances_\n",
    "\n",
    "def print_feature_importance(importances, columns):\n",
    "    feature_importances = pd.DataFrame(importances, index=columns, columns=['Importance'])\n",
    "    sorted_feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "    sorted_feature_importances.plot(kind='bar')\n",
    "    plt.title('Feature Importances')\n",
    "    plt.rcParams['figure.figsize'] = [10, 10]\n",
    "    plt.show()\n",
    "\n",
    "training_loss, validation_loss, importance = predict_random_forest(train_set, train_labels, validation_set, validation_labels, n_estimators=500, max_depth=1)\n",
    "print_feature_importance(importance, train_set.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Random Forest models, adjusting tree parameters like tree depth and the number of trees is essential for balancing learning depth and prediction stability. Deeper trees can learn more complex patterns, but may overfit, especially with small datasets. More trees improve prediction stability but increase computational load. With our small dataset, preventing overfitting solely through these adjustments is challenging and the feature importance through subsequence runs not very stable.\n",
    "\n",
    "### Stabilizing Model Predictions and Feature Importance through a larger Ensemble\n",
    "\n",
    "To improve the stability of our Random Forest model's predictions and feature importance assessments, we average results over multiple training iterations. This method reduces the influence of any specific dataset anomalies by training the model several times with varying data subsets or initial conditions. \n",
    "\n",
    "The averaging over the feature importance of multiple trainings can be considered an ensemble prediction of feature importance, just like the random forest is an ensemble prediction of the label! *Note*: Specifically for random forests, a similar effect could be achieved by increasing the number of trees in the forest. However, other model architectures which do not have a “forest” can be used with the following approach as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_average_random_forest(train_set, train_labels, validation_set, validation_labels, n_estimators, max_depth, n_iterations):\n",
    "    # Initialize accumulators for training loss and validation loss\n",
    "    total_training_loss = 0\n",
    "    total_validation_loss = 0\n",
    "    total_feature_importances = np.zeros(train_set.shape[1])\n",
    "    for _ in range(n_iterations):\n",
    "        training_loss, validation_loss, feature_importances = predict_random_forest(\n",
    "            train_set, train_labels, validation_set, validation_labels, n_estimators, max_depth\n",
    "        )\n",
    "    \n",
    "        # Accumulate the losses\n",
    "        total_training_loss += training_loss\n",
    "        total_validation_loss += validation_loss\n",
    "    \n",
    "        # Accumulate the feature importances\n",
    "        total_feature_importances += feature_importances\n",
    "    return total_training_loss / n_iterations, total_validation_loss / n_iterations,  total_feature_importances / n_iterations\n",
    "\n",
    "training_loss, validation_loss, importance = predict_average_random_forest(train_set, train_labels, validation_set, validation_labels, n_estimators=50, max_depth=1, n_iterations=5)\n",
    "print(\"Train: {}, Validiation: {}\".format(training_loss, validation_loss))\n",
    "print_feature_importance(importance, train_set.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Stabilizing Feature Importance\n",
    "\n",
    "To further enhance the stability of feature importance assessments, we average the importance scores across multiple training and testing splits. This method minimizes the impact of variations from any single dataset configuration, ensuring a more consistent and dependable evaluation of which features most significantly influence the model's predictions. This is especially important on small datasets, as small differences in training / test split can significantly change the distribution. A more systematic approach to this is k-fold cross validation. *Note*: While this does not improve the predictions of the model, it can greatly improve our “meta knowledge” about the model and training, e.g., the average amount of overfitting or the average feature importance. (Image: \"[Leave one out cross-validation animated gif for n = 8 observations.](https://commons.wikimedia.org/wiki/File:LOOCV.gif)\" by \"MBanuelos22\" under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/deed.en) license.)\n",
    "\n",
    "<div>\n",
    "<img src=\"LOOCV.gif\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_average_dataset_random_forest(train_set, train_labels, n_estimators, max_depth, tree_iterations, dataset_iterations):\n",
    "    total_training_loss = 0\n",
    "    total_validation_loss = 0\n",
    "    total_feature_importances = np.zeros(train_set.shape[1])\n",
    "    for i in range(dataset_iterations):\n",
    "        local_train_set, local_validation_set, local_train_labels, local_validation_labels = split_dataset(train_set,train_labels,test_size=0.25)\n",
    "        local_train_set = clean_dataset(local_train_set)\n",
    "        local_train_labels = clean_dataset(local_train_labels)\n",
    "        training_loss, validation_loss, importance = predict_average_random_forest(local_train_set, local_train_labels, local_validation_set, local_validation_labels, n_estimators=50, max_depth=1, n_iterations=tree_iterations)\n",
    "        # Accumulate the losses\n",
    "        total_training_loss += training_loss\n",
    "        total_validation_loss += validation_loss\n",
    "    \n",
    "        # Accumulate the feature importances\n",
    "        total_feature_importances += importance\n",
    "    return total_training_loss / dataset_iterations, total_validation_loss / dataset_iterations,  total_feature_importances / dataset_iterations \n",
    "\n",
    "training_loss, validation_loss, importance = predict_average_dataset_random_forest(train_set, train_labels, n_estimators=50, max_depth=1, tree_iterations=5, dataset_iterations=5)\n",
    "print(\"Train: {}, Validiation: {}\".format(training_loss, validation_loss))\n",
    "print_feature_importance(importance, train_set.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteratively Dropping Least Significant Features\n",
    "\n",
    "In this experiment, we systematically drop the least significant features from our model based on their importance scores to evaluate the impact on model performance. Starting with all features, we train the model, assess feature importance, and then remove the feature with the lowest importance score. This process is repeated, each time training the model with one fewer feature. This helps to establish a rank between the features. In the case of a usable model prediction, a sudden drop in model performance can also indicate an optimal number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_feature_experiment():\n",
    "    results = []\n",
    "    feature_names = train_set.columns.tolist()\n",
    "    feature_drop_iterations = {feature: [] for feature in train_set.columns}\n",
    "    \n",
    "    while len(feature_names) >= 1:\n",
    "        training_loss, validation_loss, importance = predict_average_dataset_random_forest(\n",
    "            train_set[feature_names], train_labels, n_estimators=50, max_depth=2, tree_iterations=10, dataset_iterations=10\n",
    "        )\n",
    "    \n",
    "        # Record the results\n",
    "        results.append((training_loss, validation_loss, feature_names))\n",
    "    \n",
    "        # Drop the feature with the lowest importance\n",
    "        lowest_feature = feature_names[np.argmin(importance)]\n",
    "        feature_names.remove(lowest_feature)\n",
    "        feature_drop_iterations[lowest_feature].append(len(feature_names))\n",
    "        print(\"Features left: {}. Training loss: {}. Validation loss: {}, Feature eliminated: {}\".format(len(feature_names), training_loss, validation_loss, lowest_feature))\n",
    "    return feature_drop_iterations\n",
    "\n",
    "print(drop_feature_experiment())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Feature Rankings\n",
    "\n",
    "By repeatedly executing the experiment of dropping the least significant features and averaging their ranks across multiple iterations, we can establish a stable ranking of feature importance. This method involves training the model multiple times, each time removing a feature based on its assessed importance from the previous iteration and recording the rank at which each feature is dropped.\n",
    "\n",
    "To visualize the stability and variability of these rankings, we can use box plots. Each feature's rank across different iterations is represented in a box plot, showing the median rank, the interquartile range, and any outliers. This visualization helps us understand how consistently a feature is valued across multiple models and indicates the robustness of each feature's ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_drop_feature_experiment(n_runs):\n",
    "    # Initialize a dictionary to store iteration counts for each feature\n",
    "    feature_drop_count = {}\n",
    "    \n",
    "    # Run the experiment multiple times\n",
    "    for i in range(n_runs):\n",
    "        print(f\"Experiment run {i+1}/{n_runs}\")\n",
    "        # Call the provided drop_feature_experiment function\n",
    "        current_run_results = drop_feature_experiment()\n",
    "        \n",
    "        # Accumulate results from each run\n",
    "        for feature, iterations in current_run_results.items():\n",
    "            if feature in feature_drop_count:\n",
    "                feature_drop_count[feature].extend(iterations)\n",
    "            else:\n",
    "                feature_drop_count[feature] = iterations\n",
    "    \n",
    "    return feature_drop_count\n",
    "\n",
    "def plot_drops(average_drop_iterations):\n",
    "    sorted_features = sorted(average_drop_iterations.items(), key=lambda item: np.median(item[1]))\n",
    "    features_sorted, drop_iterations_sorted = zip(*sorted_features)\n",
    "    features = list(average_drop_iterations.keys())\n",
    "    drop_iterations = [counts for counts in average_drop_iterations.values()]\n",
    "    print(features)\n",
    "    print(drop_iterations)\n",
    "    # Plotting the results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.boxplot(drop_iterations_sorted, labels=features_sorted, patch_artist=True, medianprops={'linewidth': 2, 'color': 'purple'},\n",
    "            boxprops={'facecolor': 'lightblue'}, whiskerprops={'linewidth': 2}, capprops={'linewidth': 2})\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Drop Iterations')\n",
    "    plt.title('Distribution of Drop Iterations for Each Feature')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "results = average_drop_feature_experiment(10)\n",
    "plot_drops(results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
