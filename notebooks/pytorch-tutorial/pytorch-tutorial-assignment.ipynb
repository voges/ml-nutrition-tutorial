{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PyTorch Introduction/Tutorial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will stick to a very simple and familiar problem: linear regression with a single feature $x$.\n",
    "This allows us to fully concentrate on the working of PyTorch and not be distracted by of pre- and post-processing work as necessary for problems such as image or gene classification.\n",
    "\n",
    "This is our simple linear regression model:\n",
    "\n",
    "$$y = a + bx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by generating some synthetic data.\n",
    "We generate 100 random values for our feature $x$, and we create labels using $a=1$, $b=2$ and some Gaussian noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(seed=42)  # for reproducibility\n",
    "\n",
    "x = np.random.rand(100, 1)\n",
    "a = 1\n",
    "b = 2\n",
    "y = a + b * x + 0.1 * np.random.randn(100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ **Q1** &mdash; Training and validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data randomly into a training set (80%) and a validation set (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ **Q2** &mdash; Visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the training and validations sets as scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression in NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We _know_ that $a=1$ and $b=2$, but now let’s see how close we can get to the true values by using gradient descent and the 80 points in the training set using NumPy only.\n",
    "\n",
    "We will use Mean Squared Error (MSE) as loss function $L$:\n",
    "\n",
    "$$L = \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i)^2$$\n",
    "\n",
    "Our model output $\\hat{y}_i$ for sample $i$ is defined by:\n",
    "\n",
    "$$\\hat{y}_i = a + bx_i$$\n",
    "\n",
    "This yields:\n",
    "\n",
    "$$L = \\frac{1}{N} \\sum_{i=1}^{N} (a + bx_i - y_i)^2$$\n",
    "\n",
    "The partial derivatives of $L$ w.r.t. $a$ and $b$ are:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial a} = 2 \\cdot \\frac{1}{N} \\sum_{i=1}^{N} (a + bx_i - y_i)  = 2 \\cdot \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i)$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial b} = 2 \\cdot \\frac{1}{N} \\sum_{i=1}^{N} x_i \\cdot (a + bx_i - y_i) = 2 \\cdot \\frac{1}{N} \\sum_{i=1}^{N} x_i \\cdot (\\hat{y}_i - y_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ **Q3** &mdash; Linear regression in NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters a and b randomly\n",
    "np.random.seed(seed=42)  # for reproducibility\n",
    "a = np.random.randn(1)\n",
    "b = np.random.randn(1)\n",
    "print(f\"After initialization: a={a}, b={b}\")\n",
    "\n",
    "# Set hyperparameters:\n",
    "# - Learning rate\n",
    "# - No. of epochs. An epoch is one complete iteration through the entire\n",
    "#   training dataset.\n",
    "lr = 0.1\n",
    "n_epochs = 1000\n",
    "print(f\"Learning rate: {lr}\")\n",
    "print(f\"No. of epochs: {n_epochs}\")\n",
    "\n",
    "# For each epoch, there are four training steps:\n",
    "# 1. Compute the model's predictions - forward pass\n",
    "# 2. Compute the loss, using the model's predictions and the labels and the\n",
    "#    loss function (MSE)\n",
    "# 3. Compute the partial derivatives of the loss function w.r.t. every\n",
    "#    parameter\n",
    "# 4. Update the parameters\n",
    "for epoch in range(n_epochs):\n",
    "    # 1. Compute the model's predictions - forward pass\n",
    "    # YOUR CODE\n",
    "\n",
    "    # 2. Compute the loss\n",
    "    # YOUR CODE\n",
    "\n",
    "    # 3. Compute the partial derivatives of the loss function\n",
    "    # YOUR CODE\n",
    "\n",
    "    # 4. Update the parameters\n",
    "    a = a - lr * a_grad\n",
    "    b = b - lr * b_grad\n",
    "\n",
    "    # 5. Status output\n",
    "    if epoch % 100 == 0 or epoch == n_epochs - 1:\n",
    "        print(\n",
    "            f\"Epoch [{epoch:5}/{n_epochs:5}], Loss={loss:8f}, a={a[0]:8f}, b={b[0]:8f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ **Q4** &mdash; Linear regression in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the results by comparing it to scikit-learn's [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) model.\n",
    "\n",
    "> In a LinearRegression model named `regressor`, the parameters relevant to us are accessible as `regressor.intercept_` (for $a$) and `regressor.coef_` (for $b$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, we need to convert our input data, which is in the form of NumPy arrays, into PyTorch tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, tensors are multi-dimensional arrays that are the fundamental data structures for representing and working with data.\n",
    "Tensors in PyTorch are similar to NumPy arrays, but they have the added benefit of being compatible with GPU acceleration, making them a key component for deep learning and neural network computations.\n",
    "\n",
    "One of the distinguishing features of PyTorch is its automatic differentiation capability, which is built on top of tensors.\n",
    "PyTorch keeps track of operations performed on tensors and allows you to compute gradients automatically for purposes like training neural networks using backpropagation.\n",
    "\n",
    "PyTorch tensors can be seamlessly moved to and operated on by GPUs, enabling faster computation for large-scale deep learning tasks.\n",
    "This feature is essential for training complex neural networks on GPU hardware.\n",
    "However, here we will perform all operations on CPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any case, for completeness, let's check for [CUDA](https://en.wikipedia.org/wiki/CUDA) (i.e., Nvidia GPU) support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available on your system.\")\n",
    "    device = \"gpu\"\n",
    "else:\n",
    "    print(\"CUDA is not available on your system.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's transform our training data into PyTorch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.from_numpy(x_train)\n",
    "y_train_tensor = torch.from_numpy(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what distinguishes a tensor used for data&mdash;like the ones we have just created&mdash;from a tensor used as a (trainable) parameter?\n",
    "\n",
    "The latter tensors require the computation of its gradients, so we can update their values.\n",
    "That's what the `requires_grad=True` argument is good for.\n",
    "It tells PyTorch we want it to compute gradients for us.\n",
    "\n",
    "So let's create appropriate tensors for our parameters $a$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can specify the device (CPU, GPU) at the moment of creation - recommended!\n",
    "torch.manual_seed(seed=42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "print(f\"a: {a}\")\n",
    "print(f\"b: {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to create tensors that require gradients, let's see how PyTorch handles them.\n",
    "\n",
    "[Autograd](https://pytorch.org/docs/stable/autograd.html) is PyTorch’s automatic differentiation package.\n",
    "Thanks to it, we don’t need to worry about partial derivatives, chain rule, etc. anymore.\n",
    "\n",
    "So, how do we tell PyTorch to do its thing and compute all partial derivatives?\n",
    "That’s what [`backward()`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.backward) is good for.\n",
    "([Click here for further reading on automatic differentiation.](https://en.wikipedia.org/wiki/Automatic_differentiation))\n",
    "It uses reverse-mode automatic differentiation.\n",
    "Hence, we need to invoke the `backward()` method from our `loss` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ **Q5** &mdash; A first linear regression implementation in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following code.\n",
    "Pay attention to use only PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters a and b randomly\n",
    "torch.manual_seed(seed=42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "# Set hyperparameters\n",
    "lr = 0.1\n",
    "n_epochs = 1000\n",
    "\n",
    "# For each epoch, there are four training steps\n",
    "for epoch in range(n_epochs):\n",
    "    # 1. Compute the model's predictions - forward pass\n",
    "    # YOUR CODE\n",
    "\n",
    "    # 2. Compute the loss\n",
    "    # YOUR CODE\n",
    "\n",
    "    # 3. Compute the partial derivatives of the loss function\n",
    "    # YOUR CODE\n",
    "\n",
    "    # 4. Update the parameters\n",
    "    #\n",
    "    # (PyTorch modifies computation graphs from every Python operation that\n",
    "    # involves any gradient-computing tensor or its dependencies. Hence, we\n",
    "    # need to tell it to 'back off' during our parameter update. Also, with\n",
    "    # backward(), gradients are accumulated. So, every time we use the\n",
    "    # gradients to update the parameters, we need to zero the gradients\n",
    "    # afterwards. NB: PyTorch functions with a trailing underscore - such as\n",
    "    # zero_() - perform their operations in-place.)\n",
    "    with torch.no_grad():\n",
    "        a -= lr * a.grad\n",
    "        b -= lr * b.grad\n",
    "    a.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "    # 5. Status output\n",
    "    if epoch % 100 == 0 or epoch == n_epochs - 1:\n",
    "        print(\n",
    "            f\"Epoch [{epoch:5}/{n_epochs:5}], Loss={loss:8f}, a={a[0]:8f}, b={b[0]:8f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional] Dynamic computation graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at PyTorch's dynamic computation graph.\n",
    "To visualize a graph associated with a given Python variable, we will use the [PyTorchViz](https://github.com/szagoruyko/pytorchviz) package.\n",
    "\n",
    "> The PyTorchViz package is not installed in any of the GWDG Jupyter Cloud images.\n",
    "> However, you can try this out on a local machine.\n",
    "> We provide a conda environment containing all necessary packages.\n",
    "\n",
    "Here we will take a look at the dynamic computation graph of our loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed=42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "yhat = a + b * x_train_tensor\n",
    "loss = ((y_hat - y_train_tensor) ** 2).mean()\n",
    "\n",
    "from torchviz.dot import make_dot  # pyright: ignore [reportMissingImports]\n",
    "\n",
    "make_dot(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a closer look at its components:\n",
    "\n",
    "- Blue boxes: these correspond to the tensors we use as parameters, i.e., those we arere asking PyTorch to compute gradients for\n",
    "- Gray boxes: a Python operation that involves a gradient-computing tensor or its dependencies\n",
    "- Green box: the same as the gray box, except it is the starting point for the computation of gradients (assuming the `backward()` method is called from the variable used to visualize the graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we manually updated our parameters ($a$ and $b$) using the computed gradients.\n",
    "In the case that we have many more parameters, it is more convenient to use one of PyTorch's optimizers, such as [`SGD`](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD) (Stochastic Gradient Descent).\n",
    "\n",
    "An optimizer takes the parameters we want to update, the learning rate we want to use (and possibly many other hyperparameters) and performs the updates through its [`step()`](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html#torch.optim.Optimizer.step) method.\n",
    "\n",
    "Also, we do not need to zero the gradient one by one anymore; we can use the optimizer's [`zero_grad()`](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad) method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ **Q6** &mdash; Using an optimizer in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following code using PyTorch's `SGD`.\n",
    "Pay attention to correctly pass our parameters ($a$ and $b$) (as a simple list) and the learning rate to the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters a and b randomly\n",
    "torch.manual_seed(seed=42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "# Set hyperparameters\n",
    "lr = 0.1\n",
    "n_epochs = 1000\n",
    "\n",
    "# Define an optimizer\n",
    "# YOUR CODE\n",
    "\n",
    "# For each epoch, there are four training steps\n",
    "for epoch in range(n_epochs):\n",
    "    # 1. Compute the model's predictions - forward pass\n",
    "    y_hat = a + b * x_train_tensor\n",
    "\n",
    "    # 2. Compute the loss\n",
    "    loss = ((y_hat - y_train_tensor) ** 2).mean()\n",
    "\n",
    "    # 3. Compute the partial derivatives of the loss function\n",
    "    loss.backward()\n",
    "\n",
    "    # 4. Update the parameters\n",
    "    # YOUR CODE\n",
    "\n",
    "    # 5. Status output\n",
    "    if epoch % 100 == 0 or epoch == n_epochs - 1:\n",
    "        print(\n",
    "            f\"Epoch [{epoch:5}/{n_epochs:5}], Loss={loss:8f}, a={a[0]:8f}, b={b[0]:8f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also do not need to manually calculate our loss.\n",
    "Instead, we can use PyTorch to generate a loss function for us.\n",
    "Here we will use [`MSELoss()`](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ **Q7** &mdash; Using a loss function from PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following code using `MSELoss()`.\n",
    "Note that `MSELoss()` returns the appropriately initialized loss function, which later has to be called to compute the actual loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters a and b randomly\n",
    "torch.manual_seed(seed=42)\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
    "\n",
    "# Set hyperparameters\n",
    "lr = 0.1\n",
    "n_epochs = 1000\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = torch.optim.SGD(params=[a, b], lr=lr)\n",
    "\n",
    "# Define a loss function\n",
    "# YOUR CODE\n",
    "\n",
    "# For each epoch, there are four training steps\n",
    "for epoch in range(n_epochs):\n",
    "    # 1. Compute the model's predictions - forward pass\n",
    "    y_hat = a + b * x_train_tensor\n",
    "\n",
    "    # 2. Compute the loss\n",
    "    # YOUR CODE\n",
    "\n",
    "    # 3. Compute the partial derivatives of the loss function\n",
    "    loss.backward()\n",
    "\n",
    "    # 4. Update the parameters\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 5. Status output\n",
    "    if epoch % 100 == 0 or epoch == n_epochs - 1:\n",
    "        print(\n",
    "            f\"Epoch [{epoch:5}/{n_epochs:5}], Loss={loss:8f}, a={a[0]:8f}, b={b[0]:8f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this step, the only thing left to be \"transformed\" to PyTorch code is the forward pass.\n",
    "Hence, we define our own PyTorch model.\n",
    "\n",
    "In PyTorch, a model is represented by a regular Python class that inherits from the [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module) class.\n",
    "\n",
    "The most fundamental methods it needs to implement are:\n",
    "\n",
    "- `__init__()`: it defines the parts that make up the model. In our case these are the two parameters $a$ and $b$.\n",
    "- `forward()`: it performs the actual computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualLinearRegression(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # To make \"a\" and \"b\" real parameters of the model, we need to wrap\n",
    "        # them with torch.nn.Parameter\n",
    "        self.a = torch.nn.Parameter(\n",
    "            torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "        )\n",
    "        self.b = torch.nn.Parameter(\n",
    "            torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.float:\n",
    "        return self.a + self.b * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ **Q8** &mdash; Using a custom PyTorch model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following code using our newly defined model.\n",
    "Pay attention to initialize the optimizer with the new model parameters, which can be accessed through [`parameters()`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed=42)\n",
    "\n",
    "model = # YOUR CODE\n",
    "lr = 0.1\n",
    "n_epochs = 1000\n",
    "\n",
    "optimizer = # YOUR CODE\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()  # set the model to training mode\n",
    "    y_hat = model(x_train_tensor)\n",
    "    loss = loss_fn(y_hat, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 100 == 0 or epoch == n_epochs - 1:\n",
    "        print(f\"Epoch [{epoch:5}/{n_epochs:5}], Loss={loss:8f}, {model.state_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, it can be convenient to move our training code into its own function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed=42)\n",
    "\n",
    "\n",
    "def make_train_step(\n",
    "    model: torch.nn.Module,\n",
    "    loss_fn: torch.nn.modules.loss._Loss,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    ") -> Callable:\n",
    "    def train_step(x: torch.Tensor, y: torch.Tensor) -> float:\n",
    "        model.train()\n",
    "        y_hat = model(x)\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        return loss.item()\n",
    "\n",
    "    return train_step\n",
    "\n",
    "\n",
    "lr = 0.1\n",
    "model = ManualLinearRegression()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "train_step = make_train_step(model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss = train_step(x=x_train_tensor, y=y_train_tensor)\n",
    "\n",
    "    if epoch % 100 == 0 or epoch == n_epochs - 1:\n",
    "        print(f\"Epoch [{epoch:5}/{n_epochs:5}], Loss={loss:8f}, {model.state_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate our model, we also need to convert our validation data into PyTorch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_tensor = torch.from_numpy(x_val)\n",
    "y_val_tensor = torch.from_numpy(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ **Q9** &mdash; Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the model evaluation in every epoch.\n",
    "Also, track the validation losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed=42)\n",
    "\n",
    "\n",
    "def make_train_step(\n",
    "    model: torch.nn.Module,\n",
    "    loss_fn: torch.nn.modules.loss._Loss,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    ") -> Callable:\n",
    "    def train_step(x: torch.Tensor, y: torch.Tensor) -> float:\n",
    "        model.train()\n",
    "        y_hat = model(x)\n",
    "        loss = loss_fn(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        return loss.item()\n",
    "\n",
    "    return train_step\n",
    "\n",
    "\n",
    "lr = 0.1\n",
    "model = ManualLinearRegression()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "train_step = make_train_step(model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
    "n_epochs = 1000\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss = train_step(x=x_train_tensor, y=y_train_tensor)\n",
    "    train_losses.append(loss)\n",
    "\n",
    "    if epoch % 100 == 0 or epoch == n_epochs - 1:\n",
    "        print(f\"Epoch [{epoch:5}/{n_epochs:5}], Loss={loss:8f}, {model.state_dict()}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        # YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ **Q10** &mdash; Visualization of training and validation losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can inspect the training and validation losses visually.\n",
    "Plot the training and validation losses in a single figure for the first 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
